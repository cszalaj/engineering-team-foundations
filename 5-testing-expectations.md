# Testing Expectations
Testing and ensuring quality is a full Engineering team endeavor at Tessitura. This is an area that is evolving for us as a Tessitura team, with initiatives like the Shift Left project, incorporating engineers into the test automation process, and leveraging test automation more broadly to improve efficiency. 

The following guidelines outline expectations around reviewing work, unit tests, test automation. 

# Contributors
This proposal was created by Chris Szalaj and Chris Hipschen with input from Engineering Leadership and a panel of engineers including Jon Cook, Rick Dailey, Yuri Uchida, Micah Parlato, and Josh TerAvest. 

## Proposal details

### Testing Principles
* Everything we build is built to be tested via automation. Nothing is built with the expectation that it is tested manually, either at initial release or in the future. Exceptions are made for hardware that cannot be emulated. 
* Discussion about how and to what extent a feature or defect will be tested happens during card refinement and continues throughout the development process.  This discussion includes what data might need to be created for testing, how automation will test the body of work, and whether performance, load, and security testing are required. 
* Team members are expected to test their work thoroughly to ensure all acceptance criteria are met and functioning, and unit tests are passing before the work is checked in for merging. This reduces broken builds and review returns. Engineers take pride in their work and strive to keep their returned card count as low as possible.
* All commits must have unit test coverage, and unit tests are included in pull request review and comments. Unit tests should cover a wide range of scenarios and edge cases to ensure that the code behaves correctly under various conditions. However, achieving 100% coverage is not always necessary, and the focus should be on testing critical paths and potential failure points.
* We practice "The Boy Scout Rule," (Robert C. Martin - _Clean Code_) regarding test automation. If you encounter code in your work without adequate unit, integration, or end-to-end tests, take the time to add that when you are working on the code, and leave it better than you found it. 
* Any issues that result from Security or Vulnerability testing classified as "critical" or "high" are required to be addressed before the work being scanned is considered "done."
* No pull requests are to be approved without the unit of work having adequate unit test coverage. 

### Types of Tests and Who Writes Them
* **Unit tests** are a type of software testing where individual units or components of a software application are tested in isolation to ensure that they perform as intended. The purpose of unit testing is to validate that each unit of the software performs as designed. A unit in this context is the smallest testable part of an application, often a single function or method. Most often, the author of the code is responsible for writing unit tests. Unit tests are run during the application build, and the build fails if a test doesn't pass.
* **Integration tests** are a type of software testing that focuses on verifying the interactions and interfaces between different components or modules within a system. The purpose of integration testing is to ensure that individual components, when combined, function correctly as a whole. Software Engineers write integration tests most often, and they are committed to the application repository. They run during application build. The build fails if an integration test doesn't pass.
* **Functional tests** verify that the individual components or units of a software application perform their specified functions correctly. Both Software Engineers and Quality Assurance Engineers write functional tests. One example of this is our functional API testing using Postman. Some of what we write in Playwright could be considered functional testing, but most of those tests fall under the end-to-end test category. These tests are in their own repository, and must be intentionally run outside of the build and deployment process. 
* **End-to-end tests** evaluate the entire software system from start to finish, simulating real-world scenarios and user interactions. These are the tests we write using the Playwright framework, and both Software Engineers and Quality Assurance Engineers write end-to-end tests. These tests are in their own repository, and must be intentionally run outside of the build and deployment process. 
* **Performance tests** evaluate the speed and stability of the application under known, typical use conditions. Performance testing is not intended to test a system under stress or unpredictable load. It provides baseline performance statistics and deviation from average data. Software Engineering typically writes and executes performance tests. These tests are in their own repository, and must be intentionally run outside of the build and deployment process. 
* **Load or stress tests** put a system under stress, and evaluate when the system performance starts to degrade. It also tests how the system scales to meet system demands. Software Engineering typically writes and executes performance tests. These tests are in their own repository, and must be intentionally run outside of the build and deployment process.
* **Security and vulnerability tests** are conducted with automated source code analysis tools and run in CI/CD pipelines, or are executed by engineers and external evaluators. Security test results classified as "critical" or "high" are required to be resolved before work is considered "done". 
* **Manual tests** are hands-on-keyboard test cases that are executed by a manual tester. This type of testing is helpful very early in the review process before automation might be written, and also allows for flexible application and execution. Often times manual test results or test cases help inform what cases are eventually automated, and how. Manual testing is done by Software Engineers early in the development process, and is selectively applied by the Quality Assurance team. These are the most expensive types of tests that we execute. While we have extensive manual evaluation today, we are working to rapidly decrease the amount of manual review required by the Tessitura platform. 
